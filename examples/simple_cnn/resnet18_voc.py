import torch
import torch.nn as nn
import torch.optim as optim

import tqdm
import torchvision
from torchvision import transforms
from torchvision.datasets import VOCDetection
import torchvision.datasets as datasets
import torchvision.transforms as T
import torch.nn.functional as F
import time
import sys

sys.path.append("./NanoEngine_Pytorch")
from nept import (
    translate_model_to_GraphModule,
    QRule,
    graph_module_insert_quantization_nodes,
    remove_quantization_nodes,
    ex_quantize_model_fully_encapsulated,
    sanitize_onnx_names,
    shape_inference,
)

from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as T
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.backbone_utils import resnet_fpn_backbone
from torchvision.datasets import VOCDetection
from collections import OrderedDict
import os
import time
from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork, LastLevelMaxPool
from torchvision.models.detection.backbone_utils import BackboneWithFPN
from torchvision.models import resnet18, ResNet18_Weights
from collections import OrderedDict
import xml.etree.ElementTree as ET  # ç”¨äºŽè§£æž XML æ ‡æ³¨æ–‡ä»¶
import timm

# VOC 20 ä¸ªç±»åˆ« (ç´¢å¼• 0-19)
VOC_CLASSES = (
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair',
    'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant',
    'sheep', 'sofa', 'train', 'tvmonitor'
)
CLASS_TO_IDX = {cls: i for i, cls in enumerate(VOC_CLASSES)}
NUM_CLASSES = len(VOC_CLASSES)  # 20


def voc_xml_to_multilabel(target_xml_dict):
    """
    å°† VOCDetection è¿”å›žçš„ XML å­—å…¸è½¬æ¢ä¸ºå¤šæ ‡ç­¾ Tensor (Multi-Hot Vector)ã€‚
    """
    # 1. åˆå§‹åŒ–ä¸€ä¸ªé•¿åº¦ä¸º 20 çš„é›¶å‘é‡ (Multi-Hot Vector)
    labels = torch.zeros(NUM_CLASSES, dtype=torch.float32)

    # 2. ä»Žå­—å…¸ä¸­æå–æ ¹å…ƒç´ 
    # VOCDetection è¿”å›žçš„ target å·²ç»æ˜¯ XML è§£æžåŽçš„å­—å…¸
    root = target_xml_dict['annotation']

    # 3. éåŽ†æ‰€æœ‰ç‰©ä½“
    if 'object' in root:
        objects = root['object']
        # ç¡®ä¿ objects æ˜¯ä¸€ä¸ªåˆ—è¡¨ (å•ä¸ªç‰©ä½“æ—¶å¯èƒ½ä¸æ˜¯åˆ—è¡¨)
        if not isinstance(objects, list):
            objects = [objects]

        for obj in objects:
            class_name = obj['name']
            if class_name in CLASS_TO_IDX:
                idx = CLASS_TO_IDX[class_name]
                labels[idx] = 1.0  # æ ‡è®°è¯¥ç±»åˆ«å­˜åœ¨

    return labels


# æ ‡å‡†çš„åˆ†ç±»ä»»åŠ¡ Transforms
transform_classification = transforms.Compose([
    transforms.Resize((224, 224)),  # ç»Ÿä¸€å°ºå¯¸
    # transforms.Resize((28, 28)), # ç»Ÿä¸€å°ºå¯¸
    transforms.ToTensor(),  # è½¬æ¢ä¸º Tensor (HWC -> CHW, 0-255 -> 0-1)
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],  # ImageNet å‡å€¼
        std=[0.229, 0.224, 0.225]  # ImageNet æ ‡å‡†å·®
    )
])

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# def train(model, data_loader, optimizer, criterion, epochs=1):
#     model.train()
#     start_time = time.time()
#     for epoch in range(epochs):
#         for i, (images, labels) in enumerate(data_loader):
#             images, labels = images.to(device), labels.to(device)
#             optimizer.zero_grad()
#             outputs = model(images)
#             # print(outputs.shape, labels.shape)
#
#             loss = criterion(outputs, labels)
#             loss.backward()
#             optimizer.step()
#             if (i + 1) % 100 == 0:
#                 print(f"Epoch [{epoch + 1}/{epochs}], Stip [{i + 1}/{len(data_loader)}], Loss:{loss.item():.4f}, {(time.time() - start_time):.2f}s")
#                 start_time = time.time()


def multi_loss(pred: torch.Tensor, gt: torch.Tensor, p=0.5):
    gt0 = 1 - gt
    gt1 = gt
    err = (gt - pred).pow(2)
    err_p = ((err * gt1).sum(dim=0) / (gt1.sum(dim=0) + 1)).mean()  # æ­£æ ·æœ¬ï¼Œæ¯ä¸ªç±»çš„ï¼Œæ­£æ ·æœ¬å‡è¡¡
    err_n = ((err * gt0).sum(dim=0) / (gt0.sum(dim=0) + 1)).mean()  # è´Ÿæ ·æœ¬ï¼Œæ¯ä¸ªç±»çš„ï¼Œè´Ÿæ ·æœ¬å‡è¡¡
    # err_p = (((gt - pred).pow(2) * gt).sum(dim=0) / (gt.sum(dim=0) + 1)).mean()  # æ­£æ ·æœ¬ï¼Œæ¯ä¸ªç±»çš„ï¼Œæ­£æ ·æœ¬å‡è¡¡
    # err_n = (((gt - pred).pow(2) * (1 - gt)).sum(dim=0) / ((1 - gt).sum(dim=0) + 1)).mean()  # è´Ÿæ ·æœ¬ï¼Œæ¯ä¸ªç±»çš„ï¼Œè´Ÿæ ·æœ¬å‡è¡¡
    loss = err_p * p + err_n * (1 - p)  # meanä»£è¡¨ç±»é—´å‡è¡¡ï¼Œ+ä»£è¡¨ç±»å†…å‡è¡¡
    return loss * 2


def train_one_epoch(model, data_loader, optimizer, lr_sc):
    model.train()
    start_time = time.time()
    sum_loss: float = 0.
    loss_n = 0
    tqdm_train_loader = tqdm.tqdm(enumerate(data_loader), total=len(data_loader), desc="Training")
    for i, (images, labels) in tqdm_train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)

        # print(outputs.shape, labels.shape)
        # æ­£æ ·æœ¬ = (labels >= 0.5)
        # è´Ÿæ ·æœ¬ = ~æ­£æ ·æœ¬
        #
        # loss_pos = (outputs[æ­£æ ·æœ¬] - 0.8).pow(2).mean()
        # loss_neg = (outputs[è´Ÿæ ·æœ¬] - 0.1).pow(2).mean()
        # loss = loss_pos + loss_neg

        loss = multi_loss(outputs, labels)

        # loss = criterion(outputs, labels)

        sum_loss += loss.item()
        loss_n += 1
        loss.backward()
        optimizer.step()
        lr_sc.step()
        tqdm_train_loader.set_description("Training Loss: {:.4f}".format(sum_loss / loss_n))
        # if i > 40:
        #     break
        # break

    return sum_loss / loss_n, time.time() - start_time
    # if (i + 1) % 100 == 0:
    #     print(f"Epoch [{epoch + 1}/{epochs}], Stip [{i + 1}/{len(data_loader)}], Loss:{loss.item():.4f}, {(time.time() - start_time):.2f}s")
    #     start_time = time.time()


def evaluate(model, data_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        i = 0
        for images, labels in data_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            # 1. å¯¹æ¨¡åž‹ Logits åº”ç”¨ Sigmoid æ¿€æ´»å‡½æ•°
            probabilities = torch.sigmoid(outputs)

            # 2. åº”ç”¨é˜ˆå€¼ (0.5) è½¬æ¢ä¸ºäºŒå…ƒé¢„æµ‹ (0 æˆ– 1)
            # è¿™ä¸€æ­¥å°† [B, 20] çš„è¿žç»­å€¼è½¬æ¢ä¸º [B, 20] çš„äºŒå…ƒ Tensor

            predicted = probabilities

            # 3. è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æ­£ç¡®é¢„æµ‹ï¼ˆå…¨éƒ¨ 20 ä¸ªæ ‡ç­¾éƒ½æ­£ç¡®æ‰ç®—ä¸€ä¸ªï¼‰
            # å¯¹äºŽå¤šæ ‡ç­¾ä»»åŠ¡ï¼Œé€šå¸¸ä½¿ç”¨ Exact Match Ratio (æ‰€æœ‰æ ‡ç­¾éƒ½é¢„æµ‹æ­£ç¡®) æˆ– F1 Score
            # å¦‚æžœæ‚¨æƒ³è®¡ç®— Exact Match Ratioï¼š
            total_correct_predictions = ((probabilities > 0.5).int() == labels).sum().item()  # è®¡ç®—æ‰€æœ‰æ ‡ç­¾çš„åŒ¹é…æ•°

            # æ‚¨çš„åŽŸå§‹ä»£ç å¯èƒ½æƒ³è®¡ç®—æ‰€æœ‰æ ‡ç­¾ä½ç½®çš„å‡†ç¡®çŽ‡ï¼š
            correct += total_correct_predictions
            total += labels.numel()  # Batch Size * 20
            i += 1
            if i > 100:
                break

    accuracy = 100 * correct / total
    return accuracy, images, labels, predicted


if __name__ == '__main__':
    ROOT_DIR = 'Z:/data'  # ä½ çš„ VOC æ•°æ®é›†å­˜æ”¾ç›®å½•ï¼Œä¾‹å¦‚ VOCdevkit/VOC2012

    # 1. è®­ç»ƒé›†
    train_dataset = datasets.VOCDetection(
        root=ROOT_DIR,
        year='2007',  # ä¹Ÿå¯ä»¥æ˜¯ '2007' æˆ– '2007', '2012'
        image_set='trainval',  # ä½¿ç”¨ trainval è¿›è¡Œè®­ç»ƒ
        download=False,  # å¦‚æžœå·²ä¸‹è½½ï¼Œè®¾ä¸º False
        transform=transform_classification,  # åº”ç”¨å›¾åƒè½¬æ¢
        target_transform=voc_xml_to_multilabel  # ðŸ’¥ åº”ç”¨è‡ªå®šä¹‰ Target è½¬æ¢
    )

    # 2. éªŒè¯é›†
    val_dataset = datasets.VOCDetection(
        root=ROOT_DIR,
        year='2007',
        image_set='test',
        download=False,
        transform=transform_classification,
        target_transform=voc_xml_to_multilabel
    )

    # 3. DataLoader (åˆ†ç±»ä»»åŠ¡å¯ä½¿ç”¨é»˜è®¤ collate_fn)
    BATCH_SIZE = 32
    NUM_WORKERS = 0

    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=NUM_WORKERS,
        # åˆ†ç±»ä»»åŠ¡çš„å›¾åƒå°ºå¯¸ç›¸åŒï¼Œå¯ä½¿ç”¨é»˜è®¤ collate_fn
        pin_memory=True,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=0,
    )

    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    # --- 2. æ¨¡åž‹ã€ä¼˜åŒ–å™¨è®¾ç½® ---
    model = timm.create_model('resnet50', pretrained=False, num_classes=20)
    # quantized_gm = translate_model_to_GraphModule(model)
    print("--- å›¾ç»“æž„æ¨¡åž‹ ---")
    # print(quantized_gm)
    # print(list(quantized_gm.state_dict().keys()))

    # 2. Trace the model and insert quantization nodes
    # quantized_gm = graph_module_insert_quantization_nodes(
    #     quantized_gm,
    #     customer_rules=[
    #         QRule(r"weight$", 8, 0.01, 0, True),  # è‡ªå®šä¹‰è§„åˆ™
    #         QRule(r"running_var$", 8, 0.01, 0, True),  # è‡ªå®šä¹‰è§„åˆ™
    #     ],
    # )

    print("\n--- é‡åŒ–åŽçš„æ¨¡åž‹ç»“æž„ (å®Œå…¨å°è£…) ---")
    # print(quantized_gm)
    # quantized_gm.graph.print_tabular()
    model = model.to(device)
    # ä¼˜åŒ–å™¨

    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=5e-8)

    lr_sc = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=110, eta_min=1e-6)

    # åŠ è½½æƒé‡
    model.load_state_dict(torch.load("resnet18_voc.pth"))

    print("--- å¼€å§‹é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT) ---")
    epochs = 2
    for epoch in range(epochs):
        avg_loss, time_use = train_one_epoch(model, train_loader, optimizer, lr_sc)
        print(f"Epoch [{epoch + 1}/{epochs}], Loss:{avg_loss:.4f}, {time_use:.2f}s")
        # ä¿å­˜æƒé‡
        torch.save(model.state_dict(), f"resnet18_voc.pth")
        print("Save Model: resnet18_voc.pth")

    print("\n--- è¯„ä¼° QAT æ¨¡åž‹ ---")
    accuracy, images, labels, predicted = evaluate(model, val_loader)
    print(f"Accuracy: {accuracy:f}%")

    import matplotlib.pyplot as plt
    import numpy as np

    plt.imshow(images[0].permute(1, 2, 0).cpu())
    l = np.where(labels[0].cpu().numpy() > 0.5)
    p = np.where(predicted[0].cpu().numpy() > 0.5)
    plt.title(f"GT: {l}, Pred: {p}")
    print(labels[0])
    print(predicted[0])
    print(1.0 * (predicted[0] > 0.5) == labels[0])
    plt.show()
